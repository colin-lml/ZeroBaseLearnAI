# ZeroBaseLearnAI
learn ai 

https://cloud.tencent.com/developer/article/2398347


 输入层：2个神经元（输入特征 x1 和 x2）  ：[0.5, 0.3]

 隐藏层：2个神经元（带有激活函数 sigmoid）

 输出层：1个神经元（带有激活函数 sigmoid）  --> 0.8
 
 
输入层到隐藏层的权重矩阵 W1：[0.5, 0.3], [0.2, 0.4]

隐藏层到输出层的权重向量 W2：[0.6, 0.7]

隐藏层的偏置向量 b1：[0.1, 0.2]

输出层的偏置 b2：0.3


x1  ---> hide ---> out 
			
x2  ----> hide 
      
	  
	  
前向传播
 输入到隐藏层：[0.5*0.5 + 0.3*0.2 + 0.1, 0.5*0.3 + 0.3*0.4 + 0.2] = [0.31, 0.29]

 隐藏层输出（经过 sigmoid 激活函数）：[sigmoid(0.31), sigmoid(0.29)] 
 ---> [0.57, 0.57]
 
	  
隐藏层到输出层：0.6*0.57 + 0.7*0.57 + 0.3 = 0.71

隐藏层到输出层（预测值，经过sigmoid激活函数）：sigmoid(0.71)   --> 0.67 

反向传播

损失函数对输出层输入的偏导数  
2 * (0.67 - 0.8) * sigmoid_derivative(0.71) -- >  -0.05

隐藏层偏导数：

[δ2 * 0.6 * sigmoid_derivative(0.31), δ2 * 0.7 * sigmoid_derivative(0.29)]


对于权重 W2：[δ2 * 隐藏层输出1，δ2 * 隐藏层输出2] = [-0.03, -0.04]

        对于偏置 b2：δ2 = -0.05

        对于权重 W1 和 偏置 b1，需要更复杂的计算，因为它们影响到隐藏层的输出，进而影响到输出层的输入和最终的损失。这些偏导数依赖于 δ1 和输入层的值。
		
参数更新
使用梯度下降更新参数（学习率设为 0.1）：
        更新 W2：w2 - 学习率 * 参数偏导数

        更新 b2：b2 - 学习率 * 参数偏导数

        同样地更新 W1 和 b1		

