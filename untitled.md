# 导数

导数是微积分中的核心概念之一，函数在某一点的导数就是该函数所代表的曲线在这一点上的切线斜率（不清楚的话，建议找相应教程）， 常用的导数公式有

**常数函数**

若 f(x) = C（C 为常数），则f'(x) = 0

**幂函数**

$若 (f(x) = x^n)（n 为常数），则f'(x) = n *x^{n-1}$

示例 $f(x)=x^2, 则f'(x)=2x^1$

**指数函数(暂时用不上)**

$自然指数函数：若 f(x) = e^x，则f'(x) = e^x$

$一般指数函数：若 f(x) = a^x，则f'(x)= a^x * ln(a)$



**三角函数（暂时用不上）**

* f(x) = sin (x)，则 f'(x) = cos (x)
* f(x) = cos (x)，则 f'(x) = -sin (x)

****反三角函数（暂时用不上）****



**导数的运算法则**

$设函数 f(x) 和 g(x) 均可导$

1.****和差法则****

$[f(x)+g(x)]'$ = $f'(x)+g'(x)$

**乘积法则**

$[f(x)*g(x)]'$=$f'(x)*g(x)+f(x)*g'(x)$

_特别地，若 g(x)=C 为常数则: $[f(x)*g(x)]'$=C * f'(x)+f(x)*0_

**复合函数求导法则（链式法则）**

$[f(g(x))]', 令 t=g(x)$ ；$[f(g(x))]'=f'(t)*g'(x)$

示例： $y=sin(x^2)$ 则：$cos(x^2) * 2*x^1$ 

以上是在神经网络计算过程中要使用的数学基础    

1.$u=w5*x$                                      <--- ($w5*outh1+b2=nety1=u$)

2.$z=(sigmoid(u)-o1)$              <--- $outy1=sigmoid(nety1)$ 

3.$E=\frac{1}{2}z^2$                                        <----($E=\frac{1}{2}(outy1-o1)^2$) ,$z=(outy1-o1)$

4.$E'(w5)=E'*z'*u'$

5.$E'=z=>y-o1=>(outy1-o1)$

6.$z'=(sigmoid(u)'+0)=>sigmoid(u)'=>sigmoid_derivative(outy1)$

7.$u'=x*(w5^0)=x=outh1$

8.$E'(w5)=(outy1-o1)*sigmoid_derivative(outy1)*outh1$

$E=\frac{1}{2}(outy1-o1)^2$



1. $neth1=w1*i1+b1$  记作 $U=w1*i1+b1$

2. $outh1=sigmoid(neth1)$ 记作 $K=sigmoid(neth1)$

3. $nety1=outh1*w5+b2$ 记作 $G=outh1*w5+b2$

4. $nety2=outh1*w7+b2$ 记作 $F=outh1*w7+b2$

5. $Z=sigmoid(nety1)-o1 ,  outy1=sigmoid(nety1)$

6. $T=sigmoid(nety2)−o2, outy2=sigmoid(nety2)$

7. 损失函数$Ez(x)=\frac{1}{2}x^2，Et(x)=\frac{1}{2}x^2$。总损失复合函数$E=Ez+Et$

8. $E$ 对w1的偏导数为$E(w1)'=Ez(w1)'+Et(w1)'$

9. $Ez(w1)'=Ez'*Z'*G'*K'*U'$

10. $Et(w1)'=Et'*T'*F'*K'*U'$

11. $E(w1)'=Ez'*Z'*G'*K'*U'+Et'*T'*F'*K'*U'=(Ez'*Z'*G'+Et'*T'*F')*K'∗U'$

12. $Ez'=(outy1-o1),Et'=(outy2-o2),U'=1*i1+0$

13. $Z'=sigmoid_derivative(outy1)-0,T'=sigmoid_derivative(outy1)-0,$ 

14. $F(outh1)'=(1*outh1^0)*w7+0,G(outh1)'=(1*outh1^0)*w5+0,$

15. $K'=sigmoid_derivative(outh1)$

$$
E(w1)′=((outy1−o1)*sigmoid_derivative(outy1)*w5+(outy2−o2)*sigmoid_derivative(outy2)*w7)*sigmoid_derivative(outh1)*i1
$$



$T=w5*h1$+0

$F=sigmoid(T(w5)),展开式为 F(w5)=sigmoid(w5*h1)$

$E(o1,o2)=E(o1)+0，那么E=E(o1)这里o1不是真实值而是变量F(w5)它展开式为$

$E=\frac{1}{2}(sigmoid(w5*h1)-o1)^2$要对$w5$求导,用复合函数求导法则（链式法则）,$(sigmoid(w5*h1)-o1)$当成函数$Z=(sigmoid(w5*h1)-o1)$那么E对$w5$的导数$E(w5)=2*\frac{1}{2}(sigmoid(w5*h1)-o1)*Z'$



1.$E(w5)'=(sigmoid(w5*h1)-o1)*Z'$ ,  $sigmoid(w5*h1)-o1$ 我们是知道值的，代码中Loss_derivative(outy1,o1)函数计算,简写(outy1-o1)

$E(w5)'=(outy1-o1)*Z'$

2.对$Z$求导用和差法则 $Z(w5)'=[sigmoid(w5*h1)]'-[o1]'=[sigmoid(w5*h1)]', o1$是常量导数为0，$F(w5)=sigmoid(w5*h1)$所以

$Z(w5)'=F(w5)'$

3.$E(w5)'=(outy1-o1)*F(w5)'$, $F$又嵌套一个$T=w5*h1$函数，所以$F(w5)'=sigmoid(w5*h1)'*T(w5)'$, $sigmoid(w5*h1)$的导数也是知道值的用sigmoid_derivative()函数计算,简写$S(outy1)$,同理$T(w5)'=(1*w5^0)*h1=h1$

4.最后$E(w5)'=(outy1-o1)*S(outy1)*h1$



$E(w1)=h1'*w5*o1'*E(o1)'+h1'*w7*o2'*E(o2)'=(E(o1)'*w5*o1'+E(o2)'*w7*o2')*h1'$



1. $E(w5)'=z*Z(w5)'*U(w5)'$  , $Ez$

2. $Z(w5)'=[sigmoid(w5*outh1+b2)-o1]' = ((sigmoid_derivative(outy1)-0)$

3. $U(w5)'=1*outh1+0$

4. $E(w5)'=(outy1-o1)*sigmoid_derivative(outy1)*outh1$

x


https://download.pytorch.org/libtorch/cpu/libtorch-win-shared-with-deps-2.2.0%2Bcpu.zip

https://download.pytorch.org/libtorch/cpu/libtorch-win-shared-with-deps-debug-2.2.0%2Bcpu.zip

D:\libtorch2.2.0\cpu
D:\libtorch2.2.0\debug



00000000
00011000
00011000
00011000
00011000
00011000
00011000
00000000

00000000
01111100
01111100
00001100
00001100
00001100
00001100
00000000

卷积核就是图像处理时，给定输入图像，输入图像中一个小区域中像素加权平均后成为输出图像中的每个对应像素，其中权值由一个函数定义，这个函数称为卷积核。




高斯模糊
边缘检测
锐化‌
浮雕效果

卷积概念
卷积在图像处理中主要用于特征提取、滤波和模式识别，通过卷积核（滤波器）对图像局部区域进行加权计算，生成特征图。

高斯模糊
边缘检测
锐化
浮雕效果


图片效果图




一维卷积原理
举例子来说明，假设有输入序列数据 [1, 2, 3, 4, 5] 和 一组权重数据 [1, 0, 1],做以下运算
1. [1, 2, 3] * [1, 0, 1] =  1*1+2*0+3*1 = 4
2. [2, 3, 4] * [1, 0, 1] = 2*1+3*0+4*1 = 6
3. [3, 4, 5] * [1, 0, 1] = 3*1+4*0+5*1 = 8 
输出结果：[4, 6, 8]
像这样的计算叫卷积运算
[1, 0, 1]叫卷积核,长度==3

[1, 2, 3, 4, 5]
[1, 0, 1]

[1, 2, 3, 4, 5]
   [1, 0, 1]

[1, 2, 3, 4, 5]
      [1, 0, 1]

1. 如果想要输出结果与输入数据长度一样，在原数据左右两边填充0 如下  
[0,1, 2, 3, 4, 5,0]  与 [1, 0, 1]做以下运算
1. [0, 1, 2] * [1, 0, 1] =  0*1+2*0+2*1 = 2
2. [1, 2, 3] * [1, 0, 1] =  1*1+2*0+3*1 = 4
3. [2, 3, 4] * [1, 0, 1] = 2*1+3*0+4*1 = 6
4. [3, 4, 5] * [1, 0, 1] = 3*1+4*0+5*1 = 8  
4. [4, 5, 0] * [1, 0, 1] = 4*1+5*0+0*1 = 4 
输出结果：[2, 4, 6, 8, 4]
把这个叫填充一般是填充0

2. 以上计算卷积核在原数据上一次只移动一个数据位其步长为1，如步长为2时
1. [1, 2, 3] * [1, 0, 1] =  1*1+2*0+3*1 = 4
2. [3, 4, 5] * [1, 0, 1] = 3*1+4*0+5*1 = 8 
输出结果：[4, 8]

总结：
卷积运算过程：卷积核从左往右滑动加权求和
[1, 2, 3, 4, 5]
[1, 0, 1]
[1, 2, 3, 4, 5]
   [1, 0, 1]
[1, 2, 3, 4, 5]
      [1, 0, 1]
	  
inlength：原数据长度
padding：填充位
kernelsize：卷积核大小
stride:步长
输出长度： (inlength  +  2*padding - (kernelsize-1) -1)/stride + 1

卷积





假设输入序列为 [1, 2, 3, 4, 5]（长度 = 5），卷积核为 [0.2, 0.5, 0.3]（kernel_size=3），stride=1，padding=0
卷积核覆盖 [1, 2, 3] → 计算：1*0.2 + 2*0.5 + 3*0.3 = 0.2 + 1 + 0.9 = 2.1
滑动 1 步，覆盖 [2, 3, 4] → 计算：2*0.2 + 3*0.5 + 4*0.3 = 0.4 + 1.5 + 1.2 = 3.1
滑动 1 步，覆盖 [3, 4, 5] → 计算：3*0.2 + 4*0.5 + 5*0.3 = 0.6 + 2 + 1.5 = 4.1
输出序列为 [2.1, 3.1, 4.1]（长度 = 3）。

out_length = floor((in_length + 2*padding - (kernel_size - 1) - 1) / stride + 1)

二维卷积原理
二维卷积运算过程和一维卷积一样它处理矩阵数据用举例子来说明
输入矩阵数据：
0,0,0,0,0,0,0,0,0
0,1, 2, 3, 4, 5,0
0,1, 2, 3, 4, 5,0
0,1, 2, 3, 4, 5,0
0,1, 2, 3, 4, 5,0
0,1, 2, 3, 4, 5,0
0,0,0,0,0,0,0,0,0

1, 0, 1
1, 0, 1
1, 0, 1

(1*1+2*0+3*1) + (1*1+2*0+3*1) + (1*1+2*0+3*1) = 12

卷积神经网络的中

池化层

池化是一种用于降维、提取局部显著特征的操作,常用方式
1.最大池化， 在池化窗口内取最大值作为输出，示例： 窗口 [2, 5, 3] ，输出 5。
2.平均池化，在池化窗口内取平均值作为输出，平滑局部特征
3.随机池化，在池化窗口内随机采样